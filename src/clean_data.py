#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
clean_data.py
---------------------------------
æ•°æ®æ¸…æ´—æ¨¡å—ï¼šä¸“é—¨è´Ÿè´£æ¸…æ´—ä» data_pipe.py æŠ“å–åˆ°çš„åŸå§‹æ–°é—»æ•°æ®

åŠŸèƒ½åŒ…æ‹¬ï¼š
- å»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
- ç»Ÿä¸€æ—¥æœŸæ ¼å¼
- æ–‡æœ¬å»é‡å’Œå»ç©º
- è¯­è¨€æ£€æµ‹å’Œè¿‡æ»¤
- æ•°æ®è´¨é‡æ£€æŸ¥

Author: ChatGPT (Market Alpha: NLP-Driven Factor Study)
"""

import re
import pandas as pd
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import html
import unicodedata


def clean_html_tags(text: str) -> str:
    """
    å»é™¤HTMLæ ‡ç­¾å’Œå®ä½“å­—ç¬¦
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        
    Returns:
        æ¸…æ´—åçš„æ–‡æœ¬
    """
    if not isinstance(text, str):
        return ""
    
    # è§£ç HTMLå®ä½“
    text = html.unescape(text)
    
    # å»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    
    # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()


def normalize_unicode(text: str) -> str:
    """
    æ ‡å‡†åŒ–Unicodeå­—ç¬¦
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        
    Returns:
        æ ‡å‡†åŒ–åçš„æ–‡æœ¬
    """
    if not isinstance(text, str):
        return ""
    
    # æ ‡å‡†åŒ–Unicodeå­—ç¬¦
    text = unicodedata.normalize('NFKC', text)
    
    # å»é™¤æ§åˆ¶å­—ç¬¦
    text = ''.join(char for char in text if unicodedata.category(char)[0] != 'C' or char in '\n\t')
    
    return text


def clean_text_content(text: str) -> str:
    """
    ç»¼åˆæ–‡æœ¬æ¸…æ´—
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        
    Returns:
        æ¸…æ´—åçš„æ–‡æœ¬
    """
    if not isinstance(text, str):
        return ""
    
    # å»é™¤HTMLæ ‡ç­¾
    text = clean_html_tags(text)
    
    # æ ‡å‡†åŒ–Unicode
    text = normalize_unicode(text)
    
    # å»é™¤å¤šä½™çš„æ¢è¡Œç¬¦å’Œç©ºæ ¼
    text = re.sub(r'\n\s*\n', '\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    
    # å»é™¤é¦–å°¾ç©ºç™½
    text = text.strip()
    
    return text


def normalize_datetime(date_str: str, time_str: str = None) -> Optional[datetime]:
    """
    ç»Ÿä¸€æ—¥æœŸæ—¶é—´æ ¼å¼
    
    Args:
        date_str: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
        time_str: æ—¶é—´å­—ç¬¦ä¸² (HH:MM:SS)
        
    Returns:
        æ ‡å‡†åŒ–çš„datetimeå¯¹è±¡ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å›None
    """
    try:
        if time_str and str(time_str).lower() not in ['nan', 'none', '']:
            datetime_str = f"{date_str} {time_str}"
            return datetime.strptime(datetime_str, "%Y-%m-%d %H:%M:%S")
        else:
            return datetime.strptime(date_str, "%Y-%m-%d")
    except (ValueError, TypeError):
        return None


def detect_language(text: str) -> str:
    """
    ç®€å•çš„è¯­è¨€æ£€æµ‹ï¼ˆåŸºäºå­—ç¬¦ç±»å‹ï¼‰
    
    Args:
        text: æ–‡æœ¬å†…å®¹
        
    Returns:
        è¯­è¨€ä»£ç ï¼š'zh' (ä¸­æ–‡), 'en' (è‹±æ–‡), 'ko' (éŸ©æ–‡), 'other' (å…¶ä»–)
    """
    if not isinstance(text, str) or len(text) < 10:
        return 'other'
    
    # ç»Ÿè®¡ä¸åŒå­—ç¬¦ç±»å‹çš„æ•°é‡
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    korean_chars = len(re.findall(r'[\uac00-\ud7af]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    total_chars = len(text)
    
    # è®¡ç®—æ¯”ä¾‹
    chinese_ratio = chinese_chars / total_chars
    korean_ratio = korean_chars / total_chars
    english_ratio = english_chars / total_chars
    
    # åˆ¤æ–­ä¸»è¦è¯­è¨€
    if chinese_ratio > 0.3:
        return 'zh'
    elif korean_ratio > 0.3:
        return 'ko'
    elif english_ratio > 0.5:
        return 'en'
    else:
        return 'other'


def remove_duplicates(df: pd.DataFrame, subset: List[str] = None) -> pd.DataFrame:
    """
    å»é™¤é‡å¤è®°å½•
    
    Args:
        df: æ•°æ®æ¡†
        subset: ç”¨äºåˆ¤æ–­é‡å¤çš„åˆ—ååˆ—è¡¨
        
    Returns:
        å»é‡åçš„æ•°æ®æ¡†
    """
    if subset is None:
        subset = ['uri', 'url', 'title']
    
    # åŸºäºæŒ‡å®šåˆ—å»é‡
    df_clean = df.drop_duplicates(subset=subset, keep='first')
    
    # è®°å½•å»é‡ä¿¡æ¯
    removed_count = len(df) - len(df_clean)
    if removed_count > 0:
        logging.info(f"å»é™¤äº† {removed_count} æ¡é‡å¤è®°å½•")
    
    return df_clean


def validate_data_quality(df: pd.DataFrame) -> Dict[str, Any]:
    """
    æ•°æ®è´¨é‡æ£€æŸ¥
    
    Args:
        df: æ•°æ®æ¡†
        
    Returns:
        è´¨é‡æ£€æŸ¥æŠ¥å‘Š
    """
    report = {
        'total_rows': len(df),
        'missing_values': {},
        'empty_content': 0,
        'invalid_dates': 0,
        'language_distribution': {}
    }
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    for col in df.columns:
        missing_count = df[col].isna().sum()
        if missing_count > 0:
            report['missing_values'][col] = missing_count
            # å¯¹äºæ–‡æœ¬åˆ—ï¼Œç”¨ç©ºå­—ç¬¦ä¸²å¡«å……NaN
            if col in ['body', 'title', 'summary']:
                df[col] = df[col].fillna('')
            # å¯¹äºæ•°å€¼åˆ—ï¼Œç”¨0å¡«å……NaN
            elif df[col].dtype in ['int64', 'float64']:
                df[col] = df[col].fillna(0)
    
    # æ£€æŸ¥ç©ºå†…å®¹
    if 'body' in df.columns:
        report['empty_content'] = (df['body'].str.len() < 10).sum()
    
    # æ£€æŸ¥æ— æ•ˆæ—¥æœŸ
    if 'date' in df.columns:
        report['invalid_dates'] = df['date'].isna().sum()
    
    # è¯­è¨€åˆ†å¸ƒ
    if 'body' in df.columns:
        languages = df['body'].apply(detect_language)
        report['language_distribution'] = languages.value_counts().to_dict()
    
    return report


def clean_news_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ¸…æ´—æ–°é—»æ•°æ®æ¡†
    
    Args:
        df: åŸå§‹æ•°æ®æ¡†
        
    Returns:
        æ¸…æ´—åçš„æ•°æ®æ¡†
    """
    logging.info(f"å¼€å§‹æ¸…æ´—æ•°æ®ï¼ŒåŸå§‹è®°å½•æ•°: {len(df)}")
    
    # åˆ›å»ºæ•°æ®å‰¯æœ¬
    df_clean = df.copy()
    
    # 1. æ¸…æ´—æ–‡æœ¬å†…å®¹
    text_columns = ['title', 'body']
    for col in text_columns:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].apply(clean_text_content)
            logging.info(f"æ¸…æ´—äº† {col} åˆ—")
    
    # 2. ç»Ÿä¸€æ—¥æœŸæ ¼å¼
    if 'date' in df_clean.columns and 'time' in df_clean.columns:
        df_clean['datetime_clean'] = df_clean.apply(
            lambda row: normalize_datetime(row['date'], row['time']), axis=1
        )
        logging.info("ç»Ÿä¸€äº†æ—¥æœŸæ—¶é—´æ ¼å¼")
    
    # 3. æ·»åŠ è¯­è¨€æ£€æµ‹
    if 'body' in df_clean.columns:
        df_clean['detected_lang'] = df_clean['body'].apply(detect_language)
        logging.info("æ·»åŠ äº†è¯­è¨€æ£€æµ‹")
    
    # 4. å»é™¤é‡å¤è®°å½•
    df_clean = remove_duplicates(df_clean)
    
    # 5. è¿‡æ»¤ç©ºå†…å®¹
    if 'body' in df_clean.columns:
        before_count = len(df_clean)
        df_clean = df_clean[df_clean['body'].str.len() >= 10]
        after_count = len(df_clean)
        if before_count != after_count:
            logging.info(f"è¿‡æ»¤äº† {before_count - after_count} æ¡ç©ºå†…å®¹è®°å½•")
    
    # 6. æ•°æ®è´¨é‡æ£€æŸ¥
    quality_report = validate_data_quality(df_clean)
    logging.info(f"æ•°æ®è´¨é‡æŠ¥å‘Š: {quality_report}")
    
    logging.info(f"æ¸…æ´—å®Œæˆï¼Œæœ€ç»ˆè®°å½•æ•°: {len(df_clean)}")
    
    return df_clean


def clean_and_save_news(
    input_file: str,
    output_file: str = None,
    output_dir: str = "data/processed",
    file_type: str = "csv"
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    æ¸…æ´—æ–°é—»æ•°æ®å¹¶ä¿å­˜
    
    Args:
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
        output_dir: è¾“å‡ºç›®å½•
        file_type: è¾“å‡ºæ–‡ä»¶ç±»å‹ ("csv" æˆ– "json")
        
    Returns:
        (æ¸…æ´—åçš„æ•°æ®æ¡†, è´¨é‡æŠ¥å‘Š)
    """
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not Path(input_file).exists():
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
    
    logging.info(f"è¯»å–è¾“å…¥æ–‡ä»¶: {input_file}")
    
    # è¯»å–æ•°æ®
    try:
        if input_file.endswith('.csv'):
            df = pd.read_csv(input_file)
        elif input_file.endswith('.jsonl'):
            df = pd.read_json(input_file, lines=True)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {input_file}")
    except Exception as e:
        raise ValueError(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
    
    # æ¸…æ´—æ•°æ®
    df_clean = clean_news_dataframe(df)
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    if output_file is None:
        input_name = Path(input_file).stem
        output_file = f"{input_name}_cleaned.{file_type}"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ¸…æ´—åçš„æ•°æ®
    full_output_path = output_path / output_file
    
    try:
        if file_type == "csv":
            df_clean.to_csv(full_output_path, index=False, encoding='utf-8')
        elif file_type == "json":
            df_clean.to_json(full_output_path, orient='records', lines=True, force_ascii=False)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {file_type}")
        
        logging.info(f"æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°: {full_output_path}")
        
    except Exception as e:
        raise ValueError(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
    
    # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
    quality_report = validate_data_quality(df_clean)
    
    return df_clean, quality_report


def main():
    """
    å‘½ä»¤è¡Œå…¥å£å‡½æ•°
    """
    import argparse
    
    parser = argparse.ArgumentParser(description="æ¸…æ´—æ–°é—»æ•°æ®")
    parser.add_argument("--input", "-i", required=True, help="è¾“å…¥æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶å")
    parser.add_argument("--output_dir", "-d", default="data/processed", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--format", "-f", choices=["csv", "json"], default="csv", help="è¾“å‡ºæ ¼å¼")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†æ—¥å¿—")
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s %(levelname)s %(message)s',
        handlers=[logging.StreamHandler()]
    )
    
    try:
        # æ‰§è¡Œæ¸…æ´—
        df_clean, quality_report = clean_and_save_news(
            input_file=args.input,
            output_file=args.output,
            output_dir=args.output_dir,
            file_type=args.format
        )
        
        print(f"\nâœ… æ•°æ®æ¸…æ´—å®Œæˆ!")
        print(f"ğŸ“Š å¤„ç†è®°å½•æ•°: {quality_report['total_rows']}")
        print(f"ğŸŒ è¯­è¨€åˆ†å¸ƒ: {quality_report['language_distribution']}")
        if quality_report['missing_values']:
            print(f"âš ï¸  ç¼ºå¤±å€¼: {quality_report['missing_values']}")
        
    except Exception as e:
        logging.error(f"æ¸…æ´—å¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())