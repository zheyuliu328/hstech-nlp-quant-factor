#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
analyze_factors.py
---------------------------------
åˆ†æžæƒ…æ„Ÿå› å­è¡¨çŽ°çš„è¯¦ç»†è„šæœ¬

åŠŸèƒ½åŒ…æ‹¬ï¼š
- åŠ è½½å› å­æ•°æ®å’ŒICç»“æžœ
- ç”Ÿæˆè¯¦ç»†çš„ç»Ÿè®¡æŠ¥å‘Š
- å¯è§†åŒ–å› å­è¡¨çŽ°

Author: ChatGPT (Market Alpha: NLP-Driven Factor Study)
"""

import pandas as pd
import numpy as np
import json
import logging
import argparse
from pathlib import Path
from typing import Dict, Any

# --- é…ç½® ---
DEFAULT_FACTOR_FILE = 'data/processed/daily_sentiment_factors.csv'
DEFAULT_IC_FILE = 'data/processed/ic_results.csv'
DEFAULT_EVAL_FILE = 'data/processed/factor_evaluation.json'
# --- ç»“æŸé…ç½® ---

def load_factor_data(factor_file: str) -> pd.DataFrame:
    """
    åŠ è½½å› å­æ•°æ®
    
    Args:
        factor_file: å› å­æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        å› å­æ•°æ®æ¡†
    """
    logging.info(f"åŠ è½½å› å­æ•°æ®: {factor_file}")
    
    if not Path(factor_file).exists():
        raise FileNotFoundError(f"å› å­æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {factor_file}")
    
    df = pd.read_csv(factor_file, encoding='utf-8-sig')
    df['date'] = pd.to_datetime(df['date'])
    
    logging.info(f"æˆåŠŸåŠ è½½ {len(df)} æ¡å› å­è®°å½•")
    return df


def load_ic_data(ic_file: str) -> pd.DataFrame:
    """
    åŠ è½½ICæ•°æ®
    
    Args:
        ic_file: ICæ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        ICæ•°æ®æ¡†
    """
    logging.info(f"åŠ è½½ICæ•°æ®: {ic_file}")
    
    if not Path(ic_file).exists():
        raise FileNotFoundError(f"ICæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {ic_file}")
    
    df = pd.read_csv(ic_file, encoding='utf-8-sig')
    df['date'] = pd.to_datetime(df['date'])
    
    logging.info(f"æˆåŠŸåŠ è½½ {len(df)} æ¡ICè®°å½•")
    return df


def load_evaluation_data(eval_file: str) -> Dict[str, Any]:
    """
    åŠ è½½è¯„ä¼°æ•°æ®
    
    Args:
        eval_file: è¯„ä¼°æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        è¯„ä¼°ç»“æžœå­—å…¸
    """
    logging.info(f"åŠ è½½è¯„ä¼°æ•°æ®: {eval_file}")
    
    if not Path(eval_file).exists():
        raise FileNotFoundError(f"è¯„ä¼°æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {eval_file}")
    
    with open(eval_file, 'r', encoding='utf-8') as f:
        eval_data = json.load(f)
    
    logging.info("æˆåŠŸåŠ è½½è¯„ä¼°æ•°æ®")
    return eval_data


def analyze_factor_distribution(factors_df: pd.DataFrame) -> Dict[str, Any]:
    """
    åˆ†æžå› å­åˆ†å¸ƒ
    
    Args:
        factors_df: å› å­æ•°æ®æ¡†
        
    Returns:
        åˆ†å¸ƒåˆ†æžç»“æžœ
    """
    logging.info("åˆ†æžå› å­åˆ†å¸ƒ...")
    
    # åŸºæœ¬ç»Ÿè®¡
    sentiment_stats = factors_df['sentiment_factor'].describe()
    weighted_stats = factors_df['weighted_factor'].describe()
    news_count_stats = factors_df['news_count'].describe()
    
    # å› å­å€¼åˆ†å¸ƒ
    sentiment_positive = (factors_df['sentiment_factor'] > 0).mean()
    sentiment_negative = (factors_df['sentiment_factor'] < 0).mean()
    sentiment_neutral = (factors_df['sentiment_factor'] == 0).mean()
    
    # æ–°é—»æ•°é‡åˆ†å¸ƒ
    high_news = (factors_df['news_count'] >= 3).mean()
    medium_news = ((factors_df['news_count'] >= 2) & (factors_df['news_count'] < 3)).mean()
    low_news = (factors_df['news_count'] == 1).mean()
    
    return {
        'sentiment_stats': sentiment_stats.to_dict(),
        'weighted_stats': weighted_stats.to_dict(),
        'news_count_stats': news_count_stats.to_dict(),
        'sentiment_distribution': {
            'positive': round(sentiment_positive, 4),
            'negative': round(sentiment_negative, 4),
            'neutral': round(sentiment_neutral, 4)
        },
        'news_count_distribution': {
            'high_news': round(high_news, 4),
            'medium_news': round(medium_news, 4),
            'low_news': round(low_news, 4)
        }
    }


def analyze_ic_performance(ic_df: pd.DataFrame) -> Dict[str, Any]:
    """
    åˆ†æžICè¡¨çŽ°
    
    Args:
        ic_df: ICæ•°æ®æ¡†
        
    Returns:
        ICåˆ†æžç»“æžœ
    """
    logging.info("åˆ†æžICè¡¨çŽ°...")
    
    if ic_df.empty:
        return {'error': 'ICæ•°æ®ä¸ºç©º'}
    
    # ICæ—¶é—´åºåˆ—åˆ†æž
    ic_trend = ic_df['IC'].diff().mean() if len(ic_df) > 1 else 0
    
    # æœ€å¤§æœ€å°IC
    max_ic = ic_df['IC'].max()
    min_ic = ic_df['IC'].min()
    max_ic_date = ic_df.loc[ic_df['IC'].idxmax(), 'date']
    min_ic_date = ic_df.loc[ic_df['IC'].idxmin(), 'date']
    
    # Rank-ICåˆ†æž
    max_rank_ic = ic_df['RankIC'].max()
    min_rank_ic = ic_df['RankIC'].min()
    
    return {
        'ic_trend': round(ic_trend, 4),
        'max_ic': round(max_ic, 4),
        'min_ic': round(min_ic, 4),
        'max_ic_date': str(max_ic_date),
        'min_ic_date': str(min_ic_date),
        'max_rank_ic': round(max_rank_ic, 4),
        'min_rank_ic': round(min_rank_ic, 4),
        'ic_range': round(max_ic - min_ic, 4)
    }


def print_detailed_report(factors_df: pd.DataFrame, ic_df: pd.DataFrame, 
                         eval_data: Dict[str, Any], dist_analysis: Dict[str, Any], 
                         ic_analysis: Dict[str, Any]) -> None:
    """
    æ‰“å°è¯¦ç»†æŠ¥å‘Š
    
    Args:
        factors_df: å› å­æ•°æ®æ¡†
        ic_df: ICæ•°æ®æ¡†
        eval_data: è¯„ä¼°æ•°æ®
        dist_analysis: åˆ†å¸ƒåˆ†æžç»“æžœ
        ic_analysis: ICåˆ†æžç»“æžœ
    """
    comp = eval_data['comprehensive']
    
    print("\n" + "="*80)
    print("ðŸ“Š æƒ…æ„Ÿå› å­è¯¦ç»†åˆ†æžæŠ¥å‘Š")
    print("="*80)
    
    # æ•°æ®æ¦‚è§ˆ
    print(f"\nðŸ“ˆ æ•°æ®æ¦‚è§ˆ:")
    print(f"   â€¢ å› å­è®°å½•æ•°: {len(factors_df):,}")
    print(f"   â€¢ è¦†ç›–äº¤æ˜“æ—¥: {factors_df['date'].nunique()}")
    print(f"   â€¢ è¦†ç›–è‚¡ç¥¨æ•°: {factors_df['code'].nunique()}")
    print(f"   â€¢ ICè®¡ç®—å¤©æ•°: {comp['total_days']}")
    
    # å› å­åˆ†å¸ƒ
    print(f"\nðŸŽ¯ å› å­åˆ†å¸ƒåˆ†æž:")
    print(f"   â€¢ æƒ…æ„Ÿå› å­å‡å€¼: {dist_analysis['sentiment_stats']['mean']:.4f}")
    print(f"   â€¢ æƒ…æ„Ÿå› å­æ ‡å‡†å·®: {dist_analysis['sentiment_stats']['std']:.4f}")
    print(f"   â€¢ æƒ…æ„Ÿå› å­èŒƒå›´: [{dist_analysis['sentiment_stats']['min']:.4f}, {dist_analysis['sentiment_stats']['max']:.4f}]")
    print(f"   â€¢ æ­£å› å­æ¯”ä¾‹: {dist_analysis['sentiment_distribution']['positive']:.2%}")
    print(f"   â€¢ è´Ÿå› å­æ¯”ä¾‹: {dist_analysis['sentiment_distribution']['negative']:.2%}")
    print(f"   â€¢ ä¸­æ€§å› å­æ¯”ä¾‹: {dist_analysis['sentiment_distribution']['neutral']:.2%}")
    
    # æ–°é—»æ•°é‡åˆ†å¸ƒ
    print(f"\nðŸ“° æ–°é—»æ•°é‡åˆ†å¸ƒ:")
    print(f"   â€¢ å¹³å‡æ–°é—»æ•°: {dist_analysis['news_count_stats']['mean']:.2f}")
    print(f"   â€¢ é«˜æ–°é—»é‡æ¯”ä¾‹ (â‰¥3æ¡): {dist_analysis['news_count_distribution']['high_news']:.2%}")
    print(f"   â€¢ ä¸­æ–°é—»é‡æ¯”ä¾‹ (2æ¡): {dist_analysis['news_count_distribution']['medium_news']:.2%}")
    print(f"   â€¢ ä½Žæ–°é—»é‡æ¯”ä¾‹ (1æ¡): {dist_analysis['news_count_distribution']['low_news']:.2%}")
    
    # ICè¡¨çŽ°
    print(f"\nðŸ“Š ICè¡¨çŽ°åˆ†æž:")
    print(f"   â€¢ ICå‡å€¼: {comp['ic_mean']:.4f}")
    print(f"   â€¢ ICæ ‡å‡†å·®: {comp['ic_std']:.4f}")
    print(f"   â€¢ IC tç»Ÿè®¡é‡: {comp['ic_t_stat']:.4f}")
    print(f"   â€¢ ICä¿¡æ¯æ¯”çŽ‡: {comp['ic_ir']:.4f}")
    print(f"   â€¢ æ­£ICæ¯”ä¾‹: {comp['positive_ic_ratio']:.2%}")
    
    if 'error' not in ic_analysis:
        print(f"   â€¢ æœ€å¤§IC: {ic_analysis['max_ic']:.4f} ({ic_analysis['max_ic_date']})")
        print(f"   â€¢ æœ€å°IC: {ic_analysis['min_ic']:.4f} ({ic_analysis['min_ic_date']})")
        print(f"   â€¢ ICèŒƒå›´: {ic_analysis['ic_range']:.4f}")
    
    # Rank-ICè¡¨çŽ°
    print(f"\nðŸ“ˆ Rank-ICè¡¨çŽ°åˆ†æž:")
    print(f"   â€¢ Rank-ICå‡å€¼: {comp['rank_ic_mean']:.4f}")
    print(f"   â€¢ Rank-ICæ ‡å‡†å·®: {comp['rank_ic_std']:.4f}")
    print(f"   â€¢ Rank-IC tç»Ÿè®¡é‡: {comp['rank_ic_t_stat']:.4f}")
    print(f"   â€¢ Rank-ICä¿¡æ¯æ¯”çŽ‡: {comp['rank_ic_ir']:.4f}")
    print(f"   â€¢ æ­£Rank-ICæ¯”ä¾‹: {comp['positive_rank_ic_ratio']:.2%}")
    
    # å› å­æœ‰æ•ˆæ€§è¯„ä¼°
    print(f"\nðŸŽ¯ å› å­æœ‰æ•ˆæ€§è¯„ä¼°:")
    ic_t_stat = abs(comp['ic_t_stat'])
    rank_ic_t_stat = abs(comp['rank_ic_t_stat'])
    
    if ic_t_stat > 2.0:
        ic_effectiveness = "å¼º"
    elif ic_t_stat > 1.5:
        ic_effectiveness = "ä¸­ç­‰"
    else:
        ic_effectiveness = "å¼±"
    
    if rank_ic_t_stat > 2.0:
        rank_ic_effectiveness = "å¼º"
    elif rank_ic_t_stat > 1.5:
        rank_ic_effectiveness = "ä¸­ç­‰"
    else:
        rank_ic_effectiveness = "å¼±"
    
    print(f"   â€¢ ICæœ‰æ•ˆæ€§: {ic_effectiveness} (tç»Ÿè®¡é‡: {ic_t_stat:.2f})")
    print(f"   â€¢ Rank-ICæœ‰æ•ˆæ€§: {rank_ic_effectiveness} (tç»Ÿè®¡é‡: {rank_ic_t_stat:.2f})")
    
    # æŠ•èµ„å»ºè®®
    print(f"\nðŸ’¡ æŠ•èµ„å»ºè®®:")
    if ic_t_stat > 2.0 and comp['ic_mean'] > 0:
        print("   âœ… å› å­æ˜¾ç¤ºå¼ºæ­£å‘é¢„æµ‹èƒ½åŠ›ï¼Œå»ºè®®ç”¨äºŽé€‰è‚¡")
    elif ic_t_stat > 1.5 and comp['ic_mean'] > 0:
        print("   âš ï¸  å› å­æ˜¾ç¤ºä¸­ç­‰æ­£å‘é¢„æµ‹èƒ½åŠ›ï¼Œå¯è°¨æ…Žä½¿ç”¨")
    elif ic_t_stat > 2.0 and comp['ic_mean'] < 0:
        print("   âœ… å› å­æ˜¾ç¤ºå¼ºè´Ÿå‘é¢„æµ‹èƒ½åŠ›ï¼Œå¯ç”¨äºŽåå‘é€‰è‚¡")
    else:
        print("   âŒ å› å­é¢„æµ‹èƒ½åŠ›è¾ƒå¼±ï¼Œéœ€è¦æ›´å¤šæ•°æ®æˆ–æ”¹è¿›æ–¹æ³•")
    
    print("="*80)


def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description="åˆ†æžæƒ…æ„Ÿå› å­è¡¨çŽ°")
    parser.add_argument("--factor_file", "-f", default=DEFAULT_FACTOR_FILE, 
                       help="å› å­æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--ic_file", "-i", default=DEFAULT_IC_FILE, 
                       help="ICæ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--eval_file", "-e", default=DEFAULT_EVAL_FILE, 
                       help="è¯„ä¼°æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†æ—¥å¿—")
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s %(levelname)s %(message)s',
        handlers=[logging.StreamHandler()]
    )
    
    try:
        # 1. åŠ è½½æ•°æ®
        factors_df = load_factor_data(args.factor_file)
        ic_df = load_ic_data(args.ic_file)
        eval_data = load_evaluation_data(args.eval_file)
        
        # 2. åˆ†æžå› å­åˆ†å¸ƒ
        dist_analysis = analyze_factor_distribution(factors_df)
        
        # 3. åˆ†æžICè¡¨çŽ°
        ic_analysis = analyze_ic_performance(ic_df)
        
        # 4. æ‰“å°è¯¦ç»†æŠ¥å‘Š
        print_detailed_report(factors_df, ic_df, eval_data, dist_analysis, ic_analysis)
        
        print(f"\nâœ… å› å­åˆ†æžå®Œæˆ!")
        
    except Exception as e:
        logging.error(f"å› å­åˆ†æžå¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
